# Example configuration for Local Semantic Image Search
# Copy this file to config.yaml and customize for your needs

# Paths
data_dir: data
db_path: data/metadata.db
index_path: data/faiss.index
embeddings_path: data/embeddings.npy
thumbnails_dir: data/thumbnails

# Model settings
model_name: "hf-hub:timm/ViT-SO400M-14-SigLIP-384"  # SigLIP-2 model
pretrained: "webli"
device: "cuda"  # Use "cpu" if no GPU available
batch_size: 32  # Reduce if out of memory, increase for faster processing

# Image processing
thumbnail_size: [384, 384]
image_extensions:
  - .jpg
  - .jpeg
  - .png
  - .webp
  - .bmp
  - .gif

# FAISS index settings
embedding_dim: 1152  # SigLIP-SO400M dimension
nlist: 4096  # Number of IVF clusters (increase for larger datasets)
m_pq: 64  # PQ sub-vectors
nbits_pq: 8  # Bits per PQ code
nprobe: 32  # Clusters to search (higher = more accurate but slower)

# Search settings
top_k_ivf: 1000  # Candidates from IVF-PQ
top_k_refined: 100  # Final results after re-ranking

# Processing
num_workers: 4  # Number of worker threads
checkpoint_interval: 1000  # Save progress every N images

# Duplicate detection
# Hamming distance threshold for perceptual hash comparison
# Lower = stricter (fewer duplicates), Higher = looser (more duplicates)
# Threshold 0 = 100% identical
# Threshold 5 = ~92% similar (default)
# Threshold 10 = ~84% similar
# Threshold 15 = ~77% similar
duplicate_hash_threshold: 5
